\documentclass[12pt]{amsart}
\usepackage[margin=1.1in]{geometry}
\usepackage{graphicx}
\linespread{1.0}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\FF}{\mathbb{F}}

\newcommand{\rank}{\mathop\mathrm{rank}}
\newcommand{\supp}{\mathop\mathrm{supp}}
\newcommand{\tr}{\mathop\mathrm{tr}}
\newcommand{\Span}{\mathop\mathrm{span}}

\newcommand{\cl}[1]{\overline{#1}}
\newcommand{\Id}{\mathop\mathrm{Id}}
\newcommand{\Int}{\mathop\mathrm{int}}

\newcommand{\Exercise}[1]{\ \par\noindent\textbf{\em{[#1]}}\\}
\newcommand{\Subexe}[1]{\ \par\noindent\textbf{\em{(#1).}}~}
\newcommand{\Subsubexe}[1]{\ \par\indent\emph{(#1).}~}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}
{-\baselineskip}{0.5\baselineskip}{\bf\leftline}}
\makeatother

\begin{document}
\noindent Machine Learning \hfill Jiecao Chen \hfill ID:4746311 \\
\textsc{Homework \#2}\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Question 1} 
\subsection*{a}
Given $X$,
take $y=1$ and $W_2$ such that $W_2^TX = 0$, for any $0 \leq t \leq 1$:
\begin{equation}
\label{eq_1.a.1}
	E(tW_1 + (1-t)W_2) = (1 - \hat{y}(tW_1)) 
\end{equation}

\begin{equation}
\label{eq_1.a.2}
	tE(W_1) + (1-t)E(W_2) = tE(W_1) + (1-t)(1 - \frac{1}{1 + exp(-0)})^2 = tE(W_1) + (1 - t) (\frac{1}{2})^2
\end{equation}	
now let $W_1^TX = -N$ where $N$ is a very large number,
we have 
$$(\ref{eq_1.a.1}) \approx 1$$
and 
$$(\ref{eq_1.a.2}) \approx t + \frac{1-t}{4}$$
Take $t = 0.5$ gives
$$ (\ref{eq_1.a.1}) > (\ref{eq_1.a.2})$$
This example shows that $E(W)$ is not necessarily a convex function of $W$


\subsection*{b}
Take $y = 1$, we have
\begin{equation}
\label{eq_1.b.1}
	E(W)  = log(1 + exp(-W^TX)) 
		  = log(1 + exp(-\sum_{i=1}^Nw_ix_i))
\end{equation}
To show $(\ref{eq_1.b.1})$ a convex function of $W$, we can simply show the corresponding Hessian matrix is 
positive semi-definite:

Denote $k = 1 + exp(-W^TX)$
\begin{equation}
	\frac{\partial E(W)}{\partial w_i} = \frac{-x_i exp(-W^TX)}{k}
\end{equation}
\begin{equation}
	H_{ij} = \frac{\partial^2 E(W)}{\partial w_i \partial w_j} = exp(-W^TX) \frac{x_ix_j}{k^2}
\end{equation}
which means the Hessian matrix can be represented as
$$
	H = exp(-W^TX) A^TA
$$
where
$$
	A = (w_1~ w_2~ \ldots~ w_N)
$$
thus $H$ is positive semi-deinite, we proved what we need.


\subsection*{c}
This could be directly seen when we note that no matter $y=1$ or $y=0$, we have
$$
	E(W) = log(1 + exp(-W^TX)) 
$$
We can definte the loss function as
$$
	loss(W) = \sum_{i=1}^N log(1 + exp(-W^TX_i)) 
$$
it's a logistics regression problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Question 2}
\subsection*{a}
it's trivial to see that 
$$
	k = \sum_{j=1}^mw_jk_j 
$$
is symmetric 

when $k_j$ is symmetric for all $1 \leq j \leq m$.

Given any vector $x$
\begin{equation}
	\label{eq_2.a.1}
	x^Tkx = \sum_{j=1}^m w_jx^T k_j x
\end{equation}
	

as for any $j$, $w_j \geq 0$, $k_j$ is positive semi-definite,
thus 
$$
	w_jx^T k_j x \geq 0
$$
 we have
$$
	(\ref{eq_2.a.1}) \geq 0
$$

Which implies  $k$ is positive semi-definite, thus is a valid kernel.


\subsection*{b}
It's trival to see that $K$ is symmetric
let $M^{-1} = diag(e^{x_1^2}, e^{x_2^2}, \dots, e^{x_n^2})$
then we have
$$
	(M^{-1})^T K M^{-1} = H = A^T A
$$
where $H_{ij} = e^{2x_ix_j}$ and $A = (e^{\sqrt{2}x_1}~ e^{\sqrt{2}x_2}~ \ldots ~e^{\sqrt{2}x_n})$
which implies that $K = M^T A^T A M$, thus $K$ is positive semi-definite.
So $K$ is a valid kernel.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Question 3}



\end{document} 
